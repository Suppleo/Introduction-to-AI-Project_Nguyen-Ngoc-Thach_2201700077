{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to AI - Project (Task 2)**\n",
        "\n",
        "**Student Name:** Nguyễn Ngọc Thạch\n",
        "\n",
        "**Student ID:** 2201700077\n",
        "\n"
      ],
      "metadata": {
        "id": "VoB3xxBCvE9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This AI model is built to process the **MNIST database** using PyTorch in Google Colab.\n",
        "\n",
        "* **Description:** The MNIST (Modified National Institute of Standards and Technology) database is a collection of handwritten digits commonly used for training various image processing systems. It consists of 28x28 pixel ***grayscale*** images of ***handwritten digits from 0 to 9***.\n",
        "\n",
        "* **Size:** The dataset contains a training set of 60,000 examples and a test set of 10,000 examples."
      ],
      "metadata": {
        "id": "JIAVbEAHvyr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Import necessary libraries\n"
      ],
      "metadata": {
        "id": "UfQ52nsuwlu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time"
      ],
      "metadata": {
        "id": "P89gOOkgvEoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Set up device (CPU or GPU)\n"
      ],
      "metadata": {
        "id": "XsfgvujMwxZD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRTn7TlwuibF",
        "outputId": "b3071778-1bfa-4c22-a652-b9c76ecae005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Define hyperparameters\n"
      ],
      "metadata": {
        "id": "eWc-KqmHxVhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "num_epochs = 5"
      ],
      "metadata": {
        "id": "WHxwEYZOxWLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Load and preprocess the MNIST dataset"
      ],
      "metadata": {
        "id": "pABrrvJdxZg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "'''\n",
        "The mean and standard deviation values used in this case (0.1307 and 0.3081)\n",
        "are precalculated statistics for the MNIST dataset. These values are commonly used for normalizing the MNIST dataset.\n",
        "'''\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrcOBNg2xbRe",
        "outputId": "92901239-7e02-41bd-a32b-19ccd27bd1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 31940702.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1080642.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 9067476.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3791150.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Define the neural network architecture\n"
      ],
      "metadata": {
        "id": "8umzlvqiyzS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = nn.functional.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = nn.functional.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "EwkaBDS8yy8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Instantiate the model, loss function, and optimizer\n"
      ],
      "metadata": {
        "id": "OqyVTqoc0p90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "fndXa9D00shh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Training loop"
      ],
      "metadata": {
        "id": "gqDdiZ4T17nG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')"
      ],
      "metadata": {
        "id": "tiUiMobE1-Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Testing loop"
      ],
      "metadata": {
        "id": "pdr0Kh3m3U5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')"
      ],
      "metadata": {
        "id": "CoiKaErr3awe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Main training and testing loop\n"
      ],
      "metadata": {
        "id": "DoUJY-KN4lXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "print(f\"Total training time: {total_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNSIxjcB4nVa",
        "outputId": "33975ff3-4d15-431e-ad66-648cf42a45cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1374: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303825\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.675387\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.679875\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.504711\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.245012\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.497394\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.637266\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.600801\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.510882\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.471456\n",
            "\n",
            "Test set: Average loss: 0.0021, Accuracy: 9568/10000 (95.68%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.311235\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.274878\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.302508\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.477271\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.446788\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.508454\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.314421\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.254665\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.517069\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.290385\n",
            "\n",
            "Test set: Average loss: 0.0018, Accuracy: 9657/10000 (96.57%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.638627\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.399473\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.463373\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.224017\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.312082\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.238137\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.576363\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.324096\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.256341\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.551019\n",
            "\n",
            "Test set: Average loss: 0.0016, Accuracy: 9688/10000 (96.88%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.340424\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.323944\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.263849\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.616257\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.404514\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.392346\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.419974\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.250227\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.536109\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.424729\n",
            "\n",
            "Test set: Average loss: 0.0017, Accuracy: 9678/10000 (96.78%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.322634\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.361334\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.343661\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.262038\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.550568\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.427237\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.562015\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.358290\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.361143\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.318987\n",
            "\n",
            "Test set: Average loss: 0.0016, Accuracy: 9694/10000 (96.94%)\n",
            "\n",
            "Total training time: 938.43 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10: Summary"
      ],
      "metadata": {
        "id": "nW1aanem82T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Device: {device}\")\n",
        "print(f\"Model architecture:\\n{model}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3F_5KyW85e_",
        "outputId": "f8108894-a307-4061-9bec-0e8b265016e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Model architecture:\n",
            "Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
            "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "Total parameters: 1199882\n",
            "Trainable parameters: 1199882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "This model is a convolutional neural network (CNN) designed for processing the **MNIST database** of handwritten digits. It consists of two convolutional layers followed by dropout layers for regularization. The flattened features are passed through two fully connected layers for classification. The model has approximately 1.2 million parameters, all of which are trainable. With an **accuracy of 96.94%**, the model demonstrates strong performance in classifying handwritten digits from the MNIST dataset."
      ],
      "metadata": {
        "id": "xXw55Geg9w00"
      }
    }
  ]
}